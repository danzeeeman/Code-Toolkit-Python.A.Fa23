{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2tHOTZYrmevi"
      },
      "outputs": [],
      "source": [
        "!pip install tokenizers\n",
        "!pip install transformers[torch]\n",
        "!pip install accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hrNQeVVTsBj6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os._exit(00)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Z0cRlXC-zLX",
        "outputId": "91ab35ee-91db-4683-a738-bb5c2c89c365"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtCFlllK9qu7"
      },
      "source": [
        "# Prompt Engineering 101\n",
        "## _How to make sweet sweet prompts_\n",
        "_aka you can make loads of money from this_\n",
        "\n",
        "## Part 1 :: It's all about the tokens\n",
        "### Tokenizers\n",
        "\n",
        "_what is a tokenizer?_\n",
        "\n",
        "Tokenizers break up _words_, _parts of words_ and _phrases_ into a unique token that the software can recognize.  \n",
        "\n",
        "Some tokenizers work by splitting up a sentence or phrase based of the spaces between words but some tokenizers take a more machine centered approach and split them up by bytes.\n",
        "\n",
        "![Tokenizers](../img/Tokenizer.jpg)\n",
        "\n",
        "In the computer world everything is made of of bits and bytes. Have you heard of the phrase Megabits and Megabytes.  Some people would want you to believe they are the same thing but they aren't.\n",
        "\n",
        "- a _bit_ is a single binary number a 0 or a 1\n",
        "- a _byte_ is made up of 8 _bits_\n",
        "\n",
        "The difference between a megabit and a megabyte is 7 million bits.\n",
        "\n",
        "The way a byte level tokenizer works is that it splits up a sentence or a phrase or a paragraph by the individual bytes.  If a word is really long it takes up more bytes in memory therefore it has more tokens.  \n",
        "\n",
        "Lets look at how a tokenizer works by looking at the openAI GPT3 tokenizer and feed it some phrases: [link](https://platform.openai.com/tokenizer)\n",
        "\n",
        "![Tokenizer](../images/tokenizer.PNG)\n",
        "\n",
        "If we turn on show _token id_ we can see the unique identifier for each word in the GPT3 tokenizer.\n",
        "\n",
        "![Token ID](../images/token-id.PNG)\n",
        "\n",
        "Let run the word _somewhere_ through the system.  Some would assume that it would have a single unique identifier. But it doesn't!\n",
        "\n",
        "![Somewhere](../images/somewhere.PNG)\n",
        "\n",
        "As you can see the word is actually 3 different tokens. the letter _s_ the suffix _ome_ and the word _here_\n",
        "\n",
        "Let look at this diagram for the steps for feeding a phrase into a tokenizer and then into a model.\n",
        "\n",
        "![Tokenizer Steps](../images/tokenizer_steps.png)\n",
        "\n",
        "The goal of tokenizers is to transform natural language into something a computer can understand and perform math operations on:\n",
        "\n",
        "\n",
        "So these bit of code\n",
        "\n",
        "```\n",
        "raw_inputs = [\n",
        "    \"I've been waiting for this blog my whole life.\",\n",
        "    \"I hate this so much!\",\n",
        "]\n",
        "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"tf\")\n",
        "```\n",
        "\n",
        "will turn the phrases:\n",
        "\n",
        "- _\"I've been waiting for this blog my whole life.\"_\n",
        "- _\"I hate this so much!\"_\n",
        "\n",
        "into this Tensor that the computer can understand\n",
        "\n",
        "```\n",
        "{\n",
        "    'input_ids': <tf.Tensor: shape=(2, 16), dtype=int32, numpy=\n",
        "        array([\n",
        "            [  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,  2026,  2878,  2166,  1012,   102],\n",
        "            [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,     0,     0,     0,     0,     0,     0]\n",
        "        ], dtype=int32)>,\n",
        "    'attention_mask': <tf.Tensor: shape=(2, 16), dtype=int32, numpy=\n",
        "        array([\n",
        "            [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
        "            [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "        ], dtype=int32)>\n",
        "}\n",
        "```\n",
        "\n",
        "### _wut is a tensor?_\n",
        "\n",
        "EXPLAIN A TENSOR HERE\n",
        "\n",
        "### HOW GPUS WORK\n",
        "\n",
        "\n",
        "### What does this all Mean?\n",
        "\n",
        "I know there is a lot to unpack in the above and we won't get into that yet.  You're probably confused and wondering why I'm telling you all of this, but it will make sense shortly.\n",
        "\n",
        "Lets look at phone numbers!\n",
        "\n",
        "(Country Code)-(Area Code)-(Exchange)-(Extension)\n",
        "\n",
        "so something like:\n",
        "\n",
        "+1-917-XXX-XXXX\n",
        "\n",
        "So if you think about a phone number here in the states, it is a set of eleven numbers that make up 4 different tokens, the country code is a unique token to everyone in the States, the area code is unique token to everyone in that area, the exchange is a unique token to everyone in that exchange, and the last four digits, is your extension token in the exchange in the area code in the country code.  So that when you put those eleven digits in the correct order, it create a unique address to your phone.  \n",
        "\n",
        "For generative AI think about this tokens as the parts that make up the address of the output you want.  It's a multi-dimensional location inside the latent space of the network.\n",
        "\n",
        "### _DAN! WTF IS LATENT SPACE???_\n",
        "\n",
        "![latent space](../images/latent_space.png)\n",
        "\n",
        "![damn]( ../images/hold_up.gif)\n",
        "\n",
        "Latent Space is the multi-dimensional space that makes up the 'knowledge' of Generative AI.\n",
        "\n",
        "Think of it as a complex space where each unique item is nearby similar items. In the figure above we see all of the items of the same color grouped together.  But because it is more than 3 dimensions (its just represented in 3 dimensions here) some of the blue dots could contain information that is similar to the pink dots and are close to them as well on that 4th, 5th, or Nth dimension.\n",
        "\n",
        "Sometimes we project it down to 2D space with a dimensional reduction step to make it easier to see\n",
        "\n",
        "![tsne]( ../images/latent_space_tsne.jpg)\n",
        "\n",
        "_DAN! WTF is DIMENSIONAL REDUCTION??_\n",
        "\n",
        "Dimensional reduction is a technique used to reduce the number of features in a dataset while retaining as much of the important information as possible\n",
        "\n",
        "You can see here when we project this representation of latent space down to 2D some of the red dots are mixed in with the green dots, that is because they contain similar data points and should be near each other\n",
        "\n",
        "![laten_space_gif]( ../images/PCA_Projection_Illustration.gif)\n",
        "\n",
        "### Putting it all together\n",
        "\n",
        " Each type of generative AI has its own unique tokenizer (or well a lot of them use the same one lol) that translates the natural language prompts you supply into an array of numbers that represent the _address_ of what you are looking for inside the _model_ of the generative AI.  This address is a unique identifier for a location inside the latent space of the AI model's _knowledge_ of what it has been trained on.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MszuU-iPr5KG"
      },
      "outputs": [],
      "source": [
        "import accelerate\n",
        "import transformers\n",
        "\n",
        "transformers.__version__, accelerate.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hXbpdMHK_gSG"
      },
      "outputs": [],
      "source": [
        "!mkdir /content/drive/MyDrive/models/\n",
        "!mkdir /content/drive/MyDrive/models/xmas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_HWf8gEGxQx"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/sammcilroy/ucl_cop_christmas.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_y717a5f9sER"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import json\n",
        "def convert_csv_to_json(csv_file_path):\n",
        "    # Read CSV file\n",
        "    with open(csv_file_path, 'r') as file:\n",
        "        reader = csv.DictReader(file)\n",
        "        rows = list(reader)\n",
        "\n",
        "    # Convert CSV data to JSON\n",
        "    json_data = json.dumps(rows, indent=4)\n",
        "\n",
        "    # Save JSON data to a file (optional)\n",
        "    with open('/content/drive/MyDrive/models/xmas/All_Playlists_Combined.json', 'w') as json_file:\n",
        "        json_file.write(json_data)\n",
        "\n",
        "    return json_data\n",
        "\n",
        "# Specify the path to your CSV file\n",
        "csv_file_path = '/content/ucl_cop_christmas/collected_data/All Playlists Combined.csv'\n",
        "\n",
        "# Convert CSV to JSON\n",
        "json_data = convert_csv_to_json(csv_file_path)\n",
        "\n",
        "print(\"Conversion completed. JSON data:\")\n",
        "print(json_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "R-M63LcMAJEr"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "def remove_special_characters_and_spaces(input_string):\n",
        "    # Define a regular expression pattern to match special characters and spaces\n",
        "    pattern = r'[^a-zA-Z0-9]+'  # This pattern will keep only letters and digits\n",
        "\n",
        "    # Use the sub method to replace matches of the pattern with an empty string\n",
        "    clean_string = re.sub(pattern, '', input_string)\n",
        "\n",
        "    return clean_string\n",
        "\n",
        "def remove_special_characters(input_string):\n",
        "    # Define a regular expression pattern to match special characters and spaces\n",
        "    pattern = r'[^a-zA-Z0-9\\s]+'  # This pattern will keep only letters and digits\n",
        "\n",
        "    # Use the sub method to replace matches of the pattern with an empty string\n",
        "    clean_string = re.sub(pattern, '', input_string)\n",
        "\n",
        "    return clean_string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "cjGs9KzDmKHl"
      },
      "outputs": [],
      "source": [
        "from traitlets import traitlets\n",
        "from dateutil.parser import parse\n",
        "from datetime import datetime\n",
        "import json\n",
        "stats_file = \"/content/drive/MyDrive/models/xmas/All_Playlists_Combined.json\"\n",
        "lines = []\n",
        "with open(stats_file, 'r') as f:\n",
        "    xmas_songs = json.load(f)\n",
        "    for song in xmas_songs:\n",
        "        title = remove_special_characters(song['track_name'])\n",
        "        lyrics = re.sub(r'\\n', ' ', song['lyrics'])\n",
        "        lines.append(f\"##TITLE {title} ###LYRICS {lyrics} \\n\")\n",
        "\n",
        "    with open(f'/content/drive/MyDrive/models/xmas/All_Playlists_Combine.txt', 'w', encoding='utf-8') as f:\n",
        "          f.writelines(lines)\n",
        "          f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKqfiOVbnAt8",
        "outputId": "50564c8b-ea9c-4bb0-a528-de37aee365d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MsDjFXBB-xHj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DIqPMRkEnS3j"
      },
      "outputs": [],
      "source": [
        "!mkdir -p /content/drive/MyDrive/models/xmas/tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kThlA4LpnMLf",
        "outputId": "c1e46d45-0e88-48c2-bda3-c252055f7a5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Sleigh', 'bells', 'ring', 'are', 'you', 'listening']\n"
          ]
        }
      ],
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from pathlib import Path\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "tokenizer = Tokenizer(BPE(unk_token=\"<unk>\"))\n",
        "\n",
        "tokenizer.pre_tokenizer = Whitespace()\n",
        "\n",
        "trainer = BpeTrainer(special_tokens=[\n",
        "    \"<s>\",\n",
        "    \"<pad>\",\n",
        "    \"</s>\",\n",
        "    \"<unk>\",\n",
        "    \"<mask>\"\n",
        "    ])\n",
        "\n",
        "tokenizer.train(files=[\"/content/drive/MyDrive/models/xmas/All_Playlists_Combine.txt\"], trainer=trainer)\n",
        "tokenizer.save(\"/content/drive/MyDrive/models/xmas/tokenizer/xmas.json\")\n",
        "\n",
        "output = tokenizer.encode(\"Sleigh bells ring are you listening\")\n",
        "print(output.tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "UnFozzBin_3F"
      },
      "outputs": [],
      "source": [
        "from transformers import RobertaConfig\n",
        "\n",
        "config = RobertaConfig(\n",
        "    vocab_size=12306,\n",
        "    max_position_embeddings=514,\n",
        "    num_attention_heads=12,\n",
        "    num_hidden_layers=6,\n",
        "    type_vocab_size=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "frLPEf9qoCSx"
      },
      "outputs": [],
      "source": [
        "from transformers import RobertaTokenizerFast\n",
        "\n",
        "tokenizer = RobertaTokenizerFast(tokenizer_file=\"/content/drive/MyDrive/models/xmas/tokenizer/xmas.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bQc-nsqoJjO",
        "outputId": "3b9e9f5b-a748-4692-b809-3f5930034250"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "52979730"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import RobertaForMaskedLM\n",
        "\n",
        "model = RobertaForMaskedLM(config=config)\n",
        "model.num_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "DoTD1bX4oOgh"
      },
      "outputs": [],
      "source": [
        "from transformers import LineByLineTextDataset\n",
        "\n",
        "dataset = LineByLineTextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path=\"/content/drive/MyDrive/models/xmas/All_Playlists_Combine.txt\",\n",
        "    block_size=128,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "-2DecShuoSJ9"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "R4-YWjSEoTtY"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/models/xmas/\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=500,\n",
        "    per_device_train_batch_size=64,\n",
        "    save_steps=500,\n",
        "    save_total_limit=2,\n",
        "    prediction_loss_only=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=dataset,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6SrrqZ2Z6ZO"
      },
      "outputs": [],
      "source": [
        "trainer.train(resume_from_checkpoint='/content/drive/MyDrive/models/xmas/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LTpCxuIMawD8"
      },
      "outputs": [],
      "source": [
        "trainer.save_model(\"/content/drive/MyDrive/models/xmas/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "lVl9Ix_ePwYz",
        "outputId": "8d64e0d3-a9fe-465e-ed6b-2dc3a4c1c6a8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2000' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2000/2000 24:19, Epoch 100/100]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>5.584800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>4.259600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>3.516200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>3.130800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2000, training_loss=4.122851196289062, metrics={'train_runtime': 1460.1922, 'train_samples_per_second': 84.989, 'train_steps_per_second': 1.37, 'total_flos': 4110973722777600.0, 'train_loss': 4.122851196289062, 'epoch': 100.0})"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-aa0Bb2zUb5x"
      },
      "outputs": [],
      "source": [
        "drive.flush_and_unmount()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qgo21aWWUhBR"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 773
        },
        "id": "6OGytdepobuS",
        "outputId": "561e45fb-d27a-4171-9669-3937cfb99875"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6292' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 6292/10000 1:16:29 < 45:05, 1.37 it/s, Epoch 314.55/500]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>2.534000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>1.674300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>1.094500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.684100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.413500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.242900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.145900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.089000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.058600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.042300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>0.033300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>0.026900</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-cbc95d7b31a7>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/drive/MyDrive/models/xmas/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   1554\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1834\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1835\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1836\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1837\u001b[0m                 if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2688\u001b[0m                 \u001b[0mscaled_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2689\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2690\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2691\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2692\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_accumulation_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   1983\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1984\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1985\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1987\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_trigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "trainer.train(resume_from_checkpoint='/content/drive/MyDrive/models/xmas/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "NR-3iK7Toe--"
      },
      "outputs": [],
      "source": [
        "trainer.save_model(\"/content/drive/MyDrive/models/xmas/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzK25DHOFlkJ"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "with open(\"/content/drive/MyDrive/models/xmas/checkpoint-6000/trainer_state.json\", \"r\") as f:\n",
        "  data = json.load(f)\n",
        "\n",
        "  params = {'legend.fontsize': 'small',\n",
        "          'figure.figsize': (15, 10),\n",
        "          'axes.labelsize': 'x-small',\n",
        "          'axes.titlesize':'x-small',\n",
        "          'xtick.labelsize':'x-small',\n",
        "          'ytick.labelsize':'x-small'}\n",
        "  plt.rcParams.update(params)\n",
        "\n",
        "  loss_value = []\n",
        "  for tick in data['log_history']:\n",
        "      if 'loss' in tick:\n",
        "          loss_value.append(tick['loss'])\n",
        "\n",
        "  plt.plot(range(0, len(loss_value)), loss_value, label=f'loss', alpha=0.15)\n",
        "  plt.savefig(f\"/content/drive/MyDrive/models/xmas/loss.jpg\")\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okBznry4MypI",
        "outputId": "176ef449-b477-4c32-97f7-b0cd833f8952"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "fill_mask = pipeline(\n",
        "    \"fill-mask\",\n",
        "    model=\"/content/drive/MyDrive/models/xmas/checkpoint-7800\",\n",
        "    tokenizer=tokenizer,\n",
        "    top_k=20,\n",
        ")\n",
        "\n",
        "fill_text = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=\"/content/drive/MyDrive/models/xmas/checkpoint-7800\",\n",
        "    tokenizer=tokenizer\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7cQQNStNMW8"
      },
      "outputs": [],
      "source": [
        "fill_text(\"##TITLE Rockin Around The Christmas Tree ###LYRICS \")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
