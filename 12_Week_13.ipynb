{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "055a3dcf89bb472d84cb9473f3b4139a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e1caa04ca9444575882268b95f0dd9aa",
              "IPY_MODEL_133e60c6133e437dbd4d26d0ae31378e",
              "IPY_MODEL_c50f27390ddc420491bc764bc1178c81"
            ],
            "layout": "IPY_MODEL_56c4b4b8063b41b49625fd47c237d636"
          }
        },
        "e1caa04ca9444575882268b95f0dd9aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_034d210649d448f3bf99637d8ee36895",
            "placeholder": "​",
            "style": "IPY_MODEL_934666a61eeb48c3b902f2fa3258fe04",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "133e60c6133e437dbd4d26d0ae31378e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ba50fa899b24aa08b2167ec713a456d",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2346061f1d554e74afdfe912acaf68a2",
            "value": 3
          }
        },
        "c50f27390ddc420491bc764bc1178c81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a0829ec5b33243e89c8f47366b5f5b51",
            "placeholder": "​",
            "style": "IPY_MODEL_43712460b4f841cb96d4dbba044fd685",
            "value": " 3/3 [02:19&lt;00:00, 42.73s/it]"
          }
        },
        "56c4b4b8063b41b49625fd47c237d636": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "034d210649d448f3bf99637d8ee36895": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "934666a61eeb48c3b902f2fa3258fe04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8ba50fa899b24aa08b2167ec713a456d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2346061f1d554e74afdfe912acaf68a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a0829ec5b33243e89c8f47366b5f5b51": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43712460b4f841cb96d4dbba044fd685": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-Ci7MHk-SXf"
      },
      "source": [
        "If you're opening this Notebook on colab, you will probably need to install 🤗 Tokenizers. Uncomment the following cell and run it.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZX-L8eK29uys",
        "outputId": "d06da032-2c14-4645-8d96-b59c86c58d5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%pip install tokenizers"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (0.15.0)\n",
            "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers) (0.19.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (4.66.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (2023.7.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wglOHRFF-NIH"
      },
      "source": [
        "If you're opening this notebook locally, make sure your environment has an install from source for both those libraries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNy3BKI8_FHU"
      },
      "source": [
        "## Prepare the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6di3WY0_GPr",
        "outputId": "96914234-fed9-43c0-b291-63278c9c02eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# first off we create the data/ dir, download raw wiki-103, and finally unzip the file\n",
        "!mkdir data\n",
        "!wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip -P data\n",
        "!unzip data/wikitext-103-raw-v1.zip -d data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-22 14:16:37--  https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.108.13, 54.231.134.168, 52.216.43.160, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.108.13|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 191984949 (183M) [application/zip]\n",
            "Saving to: ‘data/wikitext-103-raw-v1.zip’\n",
            "\n",
            "wikitext-103-raw-v1 100%[===================>] 183.09M  32.2MB/s    in 5.8s    \n",
            "\n",
            "2023-11-22 14:16:44 (31.4 MB/s) - ‘data/wikitext-103-raw-v1.zip’ saved [191984949/191984949]\n",
            "\n",
            "Archive:  data/wikitext-103-raw-v1.zip\n",
            "   creating: data/wikitext-103-raw/\n",
            "  inflating: data/wikitext-103-raw/wiki.test.raw  \n",
            "  inflating: data/wikitext-103-raw/wiki.valid.raw  \n",
            "  inflating: data/wikitext-103-raw/wiki.train.raw  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmUxQKaFFxOk"
      },
      "source": [
        "## Tokenizer from scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abUu182JF4y3"
      },
      "source": [
        "First, BERT relies on WordPiece, so we instantiate a new Tokenizer with this model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0p6_7ZWEJxG"
      },
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordPiece\n",
        "\n",
        "bert_tokenizer = Tokenizer(WordPiece())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxOmMAOuGCik"
      },
      "source": [
        "Then we know that BERT preprocesses texts by removing accents and lowercasing. We also use a unicode normalizer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkKr-hZ7F7KI"
      },
      "source": [
        "from tokenizers import normalizers\n",
        "from tokenizers.normalizers import Lowercase, NFD, StripAccents\n",
        "\n",
        "bert_tokenizer.normalizer = normalizers.Sequence([NFD(), Lowercase(), StripAccents()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzONftBqGHha"
      },
      "source": [
        "The pre-tokenizer is just splitting on whitespace and punctuation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f91rOUWvGEwG"
      },
      "source": [
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "bert_tokenizer.pre_tokenizer = Whitespace()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsEmYdeeGLt3"
      },
      "source": [
        "And the post-processing uses the template we saw in the previous section:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGzt-nPLGJ6Q"
      },
      "source": [
        "from tokenizers.processors import TemplateProcessing\n",
        "\n",
        "bert_tokenizer.post_processor = TemplateProcessing(\n",
        "    single=\"[CLS] $A [SEP]\",\n",
        "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
        "    special_tokens=[\n",
        "        (\"[CLS]\", 1),\n",
        "        (\"[SEP]\", 2),\n",
        "    ],\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fi28Mkv5GRPV"
      },
      "source": [
        "We can use this tokenizer and train on it on wikitext like in the [Quicktour](https://huggingface.co/docs/tokenizers/python/latest/quicktour.html):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPphQqQoGNzi"
      },
      "source": [
        "from tokenizers.trainers import WordPieceTrainer\n",
        "\n",
        "trainer = WordPieceTrainer(\n",
        "    vocab_size=30522, special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]\n",
        ")\n",
        "\n",
        "files = [f\"data/wikitext-103-raw/wiki.{split}.raw\" for split in [\"test\", \"train\", \"valid\"]]\n",
        "bert_tokenizer.train(files, trainer)\n",
        "\n",
        "model_files = bert_tokenizer.model.save(\"data\", \"bert-wiki\")\n",
        "bert_tokenizer.model = WordPiece.from_file(*model_files, unk_token=\"[UNK]\")\n",
        "\n",
        "bert_tokenizer.save(\"data/bert-wiki.json\")"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaVtLw1bGh-v"
      },
      "source": [
        "### Decoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCZsktw5G858"
      },
      "source": [
        "On top of encoding the input texts, a `Tokenizer` also has an API for decoding, that is converting IDs generated by your model back to a text. This is done by the methods `decode()` (for one predicted text) and `decode_batch()` (for a batch of predictions).\n",
        "\n",
        "The decoder will first convert the IDs back to tokens (using the tokenizer’s vocabulary) and remove all special tokens, then join those tokens with spaces:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DYi4JrvGdNq",
        "outputId": "e98e280e-54e3-4f88-9c6b-9310dc95a218",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "output = bert_tokenizer.encode(\"Hello, y'all! How are you 😁 ?\")\n",
        "print(output.ids)\n",
        "# [1, 27462, 16, 67, 11, 7323, 5, 7510, 7268, 7989, 0, 35, 2]\n",
        "\n",
        "bert_tokenizer.decode([1, 27462, 16, 67, 11, 7323, 5, 7510, 7268, 7989, 0, 35, 2])\n",
        "# \"Hello , y ' all ! How are you ?\""
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 27462, 16, 67, 11, 7323, 5, 7510, 7268, 7989, 0, 35, 2]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"hello , y ' all ! how are you ?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDq7j5NIHjoV"
      },
      "source": [
        "If you used a model that added special characters to represent subtokens of a given “word” (like the `\"##\"` in WordPiece) you will need to customize the decoder to treat them properly. If we take our previous `bert_tokenizer` for instance the default decoing will give:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xmzSDRhHSLc",
        "outputId": "971352b0-0996-4336-adc7-90060ef29941",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "output = bert_tokenizer.encode(\"Welcome to the 🤗 Tokenizers library.\")\n",
        "print(output.tokens)\n",
        "# [\"[CLS]\", \"welcome\", \"to\", \"the\", \"[UNK]\", \"tok\", \"##eni\", \"##zer\", \"##s\", \"library\", \".\", \"[SEP]\"]\n",
        "\n",
        "bert_tokenizer.decode(output.ids)\n",
        "# \"welcome to the tok ##eni ##zer ##s library .\""
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[CLS]', 'welcome', 'to', 'the', '[UNK]', 'tok', '##eni', '##zer', '##s', 'library', '.', '[SEP]']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'welcome to the tok ##eni ##zer ##s library .'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTQpXgIZH-Y-"
      },
      "source": [
        "But by changing it to a proper decoder, we get:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4jZekYhH5uG",
        "outputId": "5c636b70-77ab-43a1-99cf-8baf4b89ba9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from tokenizers import decoders\n",
        "\n",
        "bert_tokenizer.decoder = decoders.WordPiece()\n",
        "bert_tokenizer.decode(output.ids)\n",
        "# \"welcome to the tokenizers library.\""
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'welcome to the tokenizers library.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DirAX_BxICUR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfe2c5d6-7b96-4eaf-dd6c-14a11d720aee"
      },
      "source": [
        "output = bert_tokenizer.encode(\"From fairest creatures we desire increase, That thereby beauty’s rose might never die, But as the riper should by time decrease, His tender heir mught bear his memeory: But thou, contracted to thine own bright eyes, Feed’st thy light’st flame with self-substantial fuel, Making a famine where abundance lies, Thyself thy foe, to thy sweet self too cruel. Thou that art now the world’s fresh ornament And only herald to the gaudy spring, Within thine own bud buriest thy content And, tender churl, makest waste in niggarding.Pity the world, or else this glutton be, To eat the world’s due, by the grave and thee.\")\n",
        "print(output.tokens)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[CLS]', 'from', 'faire', '##st', 'creatures', 'we', 'desire', 'increase', ',', 'that', 'thereby', 'beauty', '’', 's', 'rose', 'might', 'never', 'die', ',', 'but', 'as', 'the', 'rip', '##er', 'should', 'by', 'time', 'decrease', ',', 'his', 'tender', 'heir', 'mugh', '##t', 'bear', 'his', 'mem', '##eo', '##ry', ':', 'but', 'th', '##ou', ',', 'contracted', 'to', 'thin', '##e', 'own', 'bright', 'eyes', ',', 'feed', '’', 'st', 'thy', 'light', '’', 'st', 'flame', 'with', 'self', '-', 'substantial', 'fuel', ',', 'making', 'a', 'famine', 'where', 'abundance', 'lies', ',', 'thy', '##self', 'thy', 'fo', '##e', ',', 'to', 'thy', 'sweet', 'self', 'too', 'cruel', '.', 'th', '##ou', 'that', 'art', 'now', 'the', 'world', '’', 's', 'fresh', 'ornament', 'and', 'only', 'herald', 'to', 'the', 'gau', '##dy', 'spring', ',', 'within', 'thin', '##e', 'own', 'bud', 'bur', '##iest', 'thy', 'content', 'and', ',', 'tender', 'chur', '##l', ',', 'makes', '##t', 'waste', 'in', 'nig', '##gard', '##ing', '.', 'pity', 'the', 'world', ',', 'or', 'else', 'this', 'glut', '##ton', 'be', ',', 'to', 'eat', 'the', 'world', '’', 's', 'due', ',', 'by', 'the', 'grave', 'and', 'the', '##e', '.', '[SEP]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZD9hz5Gcm6e",
        "outputId": "0b2056ce-fefa-49be-cc93-a1d98c105338"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n",
            "Installing collected packages: tiktoken\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tiktoken-0.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")"
      ],
      "metadata": {
        "id": "jaaHaep6czRF"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoding.decode(output.ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "qX5l5hCNc4Xg",
        "outputId": "cec41736-0757-4c22-ad1b-1bed9d369e7d"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\" werFIELD js414 earn_ISpol1 drink269 boundsindow^ locations generation.UI_TEXT1controller nearly removed seats.http]* attributeplates�1 launch Living mxunalutionsritten launchshop.callback.cpp?controller ParamDevice1 logsassign workerANDeteouts promise1arityindow{$MAN eatindow{$ wing sometimesstep2009==\"1 diedL AFP\\tdef resizeanny1MAN-lineMAN266AND1assignMANLPstepBI creatures3 ParamDevice drink EuropeanWrite removedcoveryindow^ Iconavo father.Int potentiallyassign removed%%%%%%%%%%%%%%%% drugs assign1_REG workerANDeteamldebug segmentMANCLUDING father1 Living killingCheck1 warningutions tracking towardssecondsinned define3 rocket removedcovery1API\\tList Court Scaleatisf predict1assign.getItem removedcoveryindow^ developed1 attribute removedworld father removedAND3#'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !git lfs clone https://huggingface.co/TheBloke/Wizard-Vicuna-7B-Uncensored-GPTQ\n",
        "!git lfs clone https://huggingface.co/KoboldAI/LLaMA2-13B-Tiefighter\n",
        "!git lfs clone https://huggingface.co/teknium/Mistral-Trismegistus-7B"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ME2fS59igPqv",
        "outputId": "d71123ab-8a09-4ba9-ba70-1f19a90fd444"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: 'git lfs clone' is deprecated and will not be updated\n",
            "          with new flags from 'git clone'\n",
            "\n",
            "'git clone' has been updated in upstream Git to have comparable\n",
            "speeds to 'git lfs clone'.\n",
            "Cloning into 'LLaMA2-13B-Tiefighter'...\n",
            "remote: Enumerating objects: 36, done.\u001b[K\n",
            "remote: Counting objects: 100% (32/32), done.\u001b[K\n",
            "remote: Compressing objects: 100% (32/32), done.\u001b[K\n",
            "remote: Total 36 (delta 10), reused 0 (delta 0), pack-reused 4\u001b[K\n",
            "Unpacking objects: 100% (36/36), 10.28 KiB | 1.28 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How to generate text: using different decoding methods for language generation with Transformers"
      ],
      "metadata": {
        "id": "iI70tWNcgjDp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip -qqq install bitsandbytes accelerate\n",
        "# !pip install auto-gptq\n",
        "!pip install optimum"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3t2bS_6hEbv",
        "outputId": "0966bf81-bedc-41d4-9e12-0cadee0193b1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optimum\n",
            "  Downloading optimum-1.14.1-py3-none-any.whl (399 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m399.9/399.9 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting coloredlogs (from optimum)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from optimum) (1.12)\n",
            "Requirement already satisfied: transformers[sentencepiece]>=4.26.0 in /usr/local/lib/python3.10/dist-packages (from optimum) (4.35.2)\n",
            "Requirement already satisfied: torch>=1.9 in /usr/local/lib/python3.10/dist-packages (from optimum) (2.1.0+cu118)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from optimum) (23.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optimum) (1.23.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from optimum) (0.19.4)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from optimum) (2.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.8.0->optimum) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.8.0->optimum) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.8.0->optimum) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.8.0->optimum) (4.66.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.8.0->optimum) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.8.0->optimum) (4.5.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->optimum) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->optimum) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->optimum) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]>=4.26.0->optimum) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]>=4.26.0->optimum) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]>=4.26.0->optimum) (0.4.0)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]>=4.26.0->optimum) (0.1.99)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]>=4.26.0->optimum) (3.20.3)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->optimum)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->optimum) (9.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets->optimum) (0.6)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->optimum) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->optimum) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->optimum) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets->optimum) (0.70.15)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->optimum) (3.8.6)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->optimum) (1.3.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum) (1.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.8.0->optimum) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.8.0->optimum) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.8.0->optimum) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.9->optimum) (2.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->optimum) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->optimum) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets->optimum) (1.16.0)\n",
            "Installing collected packages: humanfriendly, coloredlogs, optimum\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 optimum-1.14.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import random\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "import torch\n",
        "import accelerate"
      ],
      "metadata": {
        "id": "VJFJzNucgfbp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "model_name_or_path = \"./LLaMA2-13B-Tiefighter\"\n",
        "# To use a different branch, change revision\n",
        "# For example: revision=\"main\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
        "                                             device_map=\"auto\",\n",
        "                                             trust_remote_code=True,\n",
        "                                             revision=\"main\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
        "\n",
        "prompt = \"write the plot of an action movie that takes place in 1492 AD\"\n",
        "prompt_template=f'''A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: {prompt} ASSISTANT:\n",
        "\n",
        "'''\n",
        "print(\"\\n\\n*** Generate:\")\n",
        "\n",
        "input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
        "output = model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\n",
        "print(tokenizer.decode(output[0]))\n",
        "\n",
        "# Inference can also be done using transformers' pipeline\n",
        "\n",
        "print(\"*** Pipeline:\")\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=512,\n",
        "    do_sample=True,\n",
        "    temperature=0.7,\n",
        "    top_p=0.95,\n",
        "    top_k=40,\n",
        "    repetition_penalty=1.1\n",
        ")\n",
        "\n",
        "print(pipe(prompt_template)[0]['generated_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392,
          "referenced_widgets": [
            "055a3dcf89bb472d84cb9473f3b4139a",
            "e1caa04ca9444575882268b95f0dd9aa",
            "133e60c6133e437dbd4d26d0ae31378e",
            "c50f27390ddc420491bc764bc1178c81",
            "56c4b4b8063b41b49625fd47c237d636",
            "034d210649d448f3bf99637d8ee36895",
            "934666a61eeb48c3b902f2fa3258fe04",
            "8ba50fa899b24aa08b2167ec713a456d",
            "2346061f1d554e74afdfe912acaf68a2",
            "a0829ec5b33243e89c8f47366b5f5b51",
            "43712460b4f841cb96d4dbba044fd685"
          ]
        },
        "id": "uO0iUMGZgMPO",
        "outputId": "7636c65e-93c9-41cd-cbdf-58fb388ac295"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "055a3dcf89bb472d84cb9473f3b4139a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "*** Generate:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s> A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: write the plot of an action movie that takes place in 1492 AD ASSISTANT:\n",
            "\n",
            "In 1492 AD, the Spanish monarchs, Isabella I and Ferdinand II, have just completed the Reconquista, reclaiming the Iberian Peninsula from Muslim rule. As they look towards expanding their empire, they dispatch the greatest naval fleet ever assembled, under the command of Christopher Columbus. Their mission: to sail west until they reach the Indies and claim its vast riches for Spain.\n",
            "\n",
            "Meanwhile, a secret society of Templar Knights, known as the Order of Sword and Shield, has been gathering intelligence about the upcoming expedition. They learn that Columbus is actually a double agent, working for both the Spanish crown and their own order. His true intentions are to not only find the Indies but also to uncover a long-lost temple rumored to hold an ancient artifact of unimaginable power. The artifact, known as the Eye of the Andes, is said to grant its wielder control over the entire continent.\n",
            "\n",
            "The Templars, fearing that such power could fall into the hands of the Spanish monarchs, decide to send their own agent, a skilled warrior named Alvaro, to infiltrate the expedition and steal the Eye before Columbus can reach it. As the fleet sets sail, a deadly game of cat and mouse begins aboard the three ships: the Santa Maria, the Niña, and the Pinta.\n",
            "\n",
            "Alvaro, posing as a lowly sailor, quickly earns Columbus' trust and learns of the true purpose of their journey. He also discovers that the Eye of the Andes is hidden within a massive pyramid on a small island in the Caribbean. As they near their destination, Columbus becomes increasingly suspicious of Alvaro, who must now race against time to reach the island, find the temple, and steal the Eye before Columbus or the Spanish soldiers catch him.\n",
            "\n",
            "The tension mounts as the fleet reaches the Caribbean and begins exploring various islands. During a skirmish with the native Arawaks, Columbus is wounded and left behind on a small island. Believing him dead, the Spanish soldiers continue their search for Alvaro and the Eye. Alvaro, meanwhile, finds an ally in a native woman named Kateri, who helps him navigate the\n",
            "*** Pipeline:\n"
          ]
        }
      ]
    }
  ]
}